{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Matthew Block\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape product links from multiple pages...\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu?page=2\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu?page=3\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu?page=4\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu?page=5\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu?page=6\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu?page=7\n",
      "Fetching product links from: https://nickkuchar.com/collections/oahu?page=8\n",
      "Found a total of 96 product links across all pages.\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/the-pick-3-choose-three-12x18-retro-travel-prints\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/see-oahus-north-shore-12x18-retro-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kailua-beach-park-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hanalei-bay-kauai-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/diamond-head-crater-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/on-the-beach-in-waikiki-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/koolau-mountains-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kilauea-volcano-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kaneohe-bay-sandbar-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kona-manta-rays-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/mokulua-islands-oahu-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kona-village-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/haleiwa-wahine-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/makapuu-lighthouse-oahu-12x18-hawaii-travel-print-pre-order\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/black-rock-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/na-pali-coast-kauai-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/waikiki-surf-at-queens-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/lahaina-maui-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/road-to-hana-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/canoes-waikiki-party-wave-12-x-18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/chinamans-hat-mokolii-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/spread-the-aloha-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/lanikai-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/koko-crater-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/pearl-harbor-uss-arizona-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/winter-at-waimea-bay-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/haleiwa-town-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/sunset-beach-wahine-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/ala-moana-beach-park-12x18-retro-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/maluaka-beach-maui-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kaimuki-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/waipio-valley-big-island-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/rainbow-falls-big-island-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/molokini-crater-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kona-coast-hawaii-island-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/the-pick-5-choose-five-12x18-retro-hawaii-travel-prints\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/fall-in-love-with-ko-olina-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/manoa-valley-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/olomana-three-peaks-hike-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kahala-beach-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/shipwrecks-beach-poipu-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kaena-point-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/haleakala-crater-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/big-beach-makena-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/north-shore-oahu-surf-map-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/iao-valley-maui-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hanauma-bay-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/maili-oahu-12x18-retro-hawaii-travel-print-pre-order\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/ewa-beach-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/copy-of-makena-coastline-maui-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/princeville-kauai-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kaka-ako-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/ford-island-oahu-12x18-hawaii-travel-print-pre-order\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/waimanalo-bay-12x18-hawaii-travel-print-pre-order\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/molokai-to-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/north-shore-surf-12x12-art-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hanalei-town-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hickam-harbor-oahu-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/honaunau-hawaii-island-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kalalau-lookout-koke-e-state-park-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/lanai-puu-pehe-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hanapepe-swinging-bridge-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/wahiawa-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/lanikai-beach-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/pokai-bay-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/honolulu-morning-commute-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/south-shore-oahu-surf-map-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kilauea-lighthouse-kauai-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/waialua-sugar-mill-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/mt-kaala-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/makaha-oahu-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/oahu-leeward-coast-surf-map-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/waimea-canyon-kauai-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hanapepe-town-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/sunset-beach-he-e-nalu-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/wailua-falls-kauai-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/honolua-bay-maui-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/goin-for-a-walk-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/explore-the-oregon-coast-12x18-retro-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/tokyo-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/rocky-mountain-adventure-colorado-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/new-handmade-frame-whitewash-12x18\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/wax-it-up-12x18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/sea-life-abounds-12x18-retro-hawaii-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kaaawa-valley-oahu-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/mount-hood-12x18-retro-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/mount-waialeale-kauai-12-x-18-retro-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/kalapaki-beach-12x18-hawaii-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/breckenridge-colorado-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/vail-colorado-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/two-lovers-point-guam-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hilaan-beach-guam-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/hood-river-oregon-12x18-retro-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/talofofo-falls-guam-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/beaver-creek-colorado-12x18-travel-print\n",
      "  Scraping details from: https://nickkuchar.com/collections/oahu/products/steamboat-springs-colorado-12x18-travel-print\n",
      "\n",
      "Writing scraped data to nick-kuchar_titles_and_descriptions.html...\n",
      "Successfully saved data to c:\\Projects\\MS_Data_Analytics\\Web_Mining_Applied_NLP\\P7-article-summarizer\\nick-kuchar_titles_and_descriptions.html\n"
     ]
    }
   ],
   "source": [
    "%run nick-kuchar-scraper-multipage.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Polarity Score: 0.3079\n",
      "Number of Sentences: 524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the full path to your HTML file\n",
    "html_file_path = r'nick-kuchar_titles_and_descriptions.html'\n",
    "\n",
    "try:\n",
    "    # Read the HTML content from the file\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML and extract all text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text_content = soup.get_text()\n",
    "\n",
    "    # Create a TextBlob object for NLP analysis\n",
    "    blob = TextBlob(text_content)\n",
    "\n",
    "    # Calculate sentiment polarity\n",
    "    # Polarity is a float within the range [-1.0, 1.0]\n",
    "    # where -1 is very negative, 0 is neutral, and +1 is very positive.\n",
    "    polarity = blob.sentiment.polarity\n",
    "\n",
    "    # Count the number of sentences\n",
    "    num_sentences = len(blob.sentences)\n",
    "\n",
    "    # Print the results with descriptive labels\n",
    "    print(f\"Sentiment Polarity Score: {polarity:.4f}\")\n",
    "    print(f\"Number of Sentences: {num_sentences}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {html_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Frequent Tokens and Their Frequencies:\n",
      "-------------------------------------------------\n",
      "Token: 'print'\tFrequency: 417\n",
      "Token: 'nick'\tFrequency: 132\n",
      "Token: '12x18'\tFrequency: 102\n",
      "Token: 'travel'\tFrequency: 99\n",
      "Token: 'color'\tFrequency: 97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the full path to your HTML file\n",
    "html_file_path = r'nick-kuchar_titles_and_descriptions.html'\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Read and extract text from the HTML file ---\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text_content = soup.get_text()\n",
    "\n",
    "    # --- Step 2: Load the spaCy pipeline ---\n",
    "    # This loads the small English model.\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # --- Step 3: Process the text with spaCy ---\n",
    "    doc = nlp(text_content)\n",
    "\n",
    "    # --- Step 4: Find all tokens, excluding stop words and punctuation ---\n",
    "    # We convert tokens to lower case and strip leading/trailing whitespace.\n",
    "    # We also ensure the token is not an empty string after stripping.\n",
    "    meaningful_tokens = [\n",
    "        token.text.lower().strip() \n",
    "        for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.text.strip()\n",
    "    ]\n",
    "\n",
    "    # --- Step 5: Calculate the frequency of each token ---\n",
    "    word_freq = Counter(meaningful_tokens)\n",
    "\n",
    "    # --- Step 6: Get the 5 most common tokens ---\n",
    "    most_common_tokens = word_freq.most_common(5)\n",
    "\n",
    "    # --- Step 7: Print the results ---\n",
    "    print(\"Top 5 Most Frequent Tokens and Their Frequencies:\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    for token, freq in most_common_tokens:\n",
    "        print(f\"Token: '{token}'\\tFrequency: {freq}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {html_file_path}\")\n",
    "except ImportError:\n",
    "    print(\"spaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run the following command in a cell and restart the kernel:\")\n",
    "    print(\"!python -m spacy download en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Frequent Tokens (from descriptions only):\n",
      "-----------------------------------------------------\n",
      "Token: 'print'\tFrequency: 324\n",
      "Token: 'nick'\tFrequency: 130\n",
      "Token: 'color'\tFrequency: 97\n",
      "Token: 'designed'\tFrequency: 97\n",
      "Token: 'sized'\tFrequency: 95\n"
     ]
    }
   ],
   "source": [
    "# A second version that uses only the descriptions from the HTML file, not the titles.\n",
    "\n",
    "# Define the full path to your HTML file\n",
    "html_file_path = r'nick-kuchar_titles_and_descriptions.html'\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Read and extract text from the HTML file ---\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # --- MODIFICATION: Extract text ONLY from description <p> tags ---\n",
    "    # Find all paragraph tags, which contain the descriptions.\n",
    "    description_elements = soup.find_all('p')\n",
    "    # Join the text from all description paragraphs into a single string.\n",
    "    description_text = ' '.join([p.get_text(strip=True) for p in description_elements])\n",
    "\n",
    "    # --- Step 2: Load the spaCy pipeline ---\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # --- Step 3: Process the description text with spaCy ---\n",
    "    doc = nlp(description_text)\n",
    "\n",
    "    # --- Step 4: Find all tokens, excluding stop words and punctuation ---\n",
    "    meaningful_tokens = [\n",
    "        token.text.lower().strip() \n",
    "        for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.text.strip()\n",
    "    ]\n",
    "\n",
    "    # --- Step 5: Calculate the frequency of each token ---\n",
    "    word_freq = Counter(meaningful_tokens)\n",
    "\n",
    "    # --- Step 6: Get the 5 most common tokens ---\n",
    "    most_common_tokens = word_freq.most_common(5)\n",
    "\n",
    "    # --- Step 7: Print the results ---\n",
    "    print(\"Top 5 Most Frequent Tokens (from descriptions only):\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    for token, freq in most_common_tokens:\n",
    "        print(f\"Token: '{token}'\\tFrequency: {freq}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {html_file_path}\")\n",
    "except ImportError:\n",
    "    print(\"spaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run the following command in a cell and restart the kernel:\")\n",
    "    print(\"!python -m spacy download en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Frequent Lemmas and Their Frequencies (from descriptions only):\n",
      "-----------------------------------------------------------------------\n",
      "Lemma: 'print'\tFrequency: 436\n",
      "Lemma: 'nick'\tFrequency: 130\n",
      "Lemma: 'design'\tFrequency: 103\n",
      "Lemma: 'color'\tFrequency: 100\n",
      "Lemma: 'sized'\tFrequency: 95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the full path to your HTML file\n",
    "html_file_path = r'nick-kuchar_titles_and_descriptions.html'\n",
    "\n",
    "try:\n",
    "    # --- Step 1: Read and extract text from the HTML file's descriptions ---\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract text ONLY from description <p> tags\n",
    "    description_elements = soup.find_all('p')\n",
    "    description_text = ' '.join([p.get_text(strip=True) for p in description_elements])\n",
    "\n",
    "    # --- Step 2: Load the spaCy pipeline ---\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # --- Step 3: Process the description text with spaCy ---\n",
    "    doc = nlp(description_text)\n",
    "\n",
    "    # --- Step 4: Find all lemmas, excluding stop words and punctuation ---\n",
    "    # A lemma is the base or dictionary form of a word (e.g., \"prints\" -> \"print\").\n",
    "    # We convert lemmas to lower case for consistent counting.\n",
    "    meaningful_lemmas = [\n",
    "        token.lemma_.lower().strip() \n",
    "        for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.lemma_.strip()\n",
    "    ]\n",
    "\n",
    "    # --- Step 5: Calculate the frequency of each lemma ---\n",
    "    lemma_freq = Counter(meaningful_lemmas)\n",
    "\n",
    "    # --- Step 6: Get the 5 most common lemmas ---\n",
    "    most_common_lemmas = lemma_freq.most_common(5)\n",
    "\n",
    "    # --- Step 7: Print the results ---\n",
    "    print(\"Top 5 Most Frequent Lemmas and Their Frequencies (from descriptions only):\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    for lemma, freq in most_common_lemmas:\n",
    "        print(f\"Lemma: '{lemma}'\\tFrequency: {freq}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at the specified path: {html_file_path}\")\n",
    "except ImportError:\n",
    "    print(\"spaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run the following command in a cell and restart the kernel:\")\n",
    "    print(\"!python -m spacy download en_core_web_sm\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
